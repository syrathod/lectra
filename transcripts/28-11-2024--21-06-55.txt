 It's a great pleasure for me to be here and to lecture here. I don't know if you know that, but there is really this close connection between DeepMind and UCL, which started off by two of our founders, Demis and Shane, meeting as post-doctoral fellows at UCL and then ending up founding DeepMind. So really at the very root of things, DeepMind and UCL are connected, and I think it's great that we can celebrate that and use it by putting out this lecture series and sharing these thoughts together. I will only be the first of several lectures. Here are all the lectures within the series and you will get to know these people if you stick through their lectures and they have a lot of interesting and wonderful things to say. And towards the end of this lecture, I'm going to go through the topics that they will cover and try to motivate them in the context of the larger field of deep learning. Here's the plan for the lecture. First we'll solve intelligence and then we'll do some other things. When I have the word solving intelligence here, that refers to the first part of DeepMind's mission. DeepMind's mission has these two parts. First, solve intelligence and second, use it to solve everything else. While that is, of course, an audacious mission statement, we do believe that it's a great north star to guide our research and so far it has created a lot of great momentum. So, I could talk about deep learning in a very broad sense, giving examples from all walks of life, but I'm sure you're familiar with most of those. And so I'll take a more personal view, a more deep-mind-centric view also because that's the work that I can authentically speak about. And so what I would like to do is go through three case studies of successful deep learning applications to show you the power of deep learning and to motivate you to study it further. And in these three case studies, a lot of things will come up that you might not understand at this point in time fully, but then I can assure you that the subsequent lectures will fill in those gaps and make you appreciate what's happening here. So the first case study is AlphaGo and Alpha0. The second one has a little more action to it, learning to play the game of capture the flag. And the third one goes beyond games and is about folding proteins with alpha-fold, a deep learning-based system. And it takes us into the realm of biology and science. And then finally, the last bit of the lecture I want to go over, the pieces that the subsequent lectures will deliver and put them into a greater context to tell you what's out there and why it's worth learning about these things. So let's start with solving intelligence. The hallmark of human intelligence is its generality. And nobody has expressed this in a crisper way than the science fiction author Robert A. Heinlein. So he says, a human being should be able to change a diaper, plan an invasion, butcher a hog, connoisseur, design a building, write a sonnet, balance accounts, build a wall, get a bone, comfort the dying, take orders, give orders, cooperate, act alone, solve equations, analyze a new problem, pitch manure, program a computer, cook a tasty meal, fight efficiently, die gallantly. Specialization is for insects. Now nothing against insects because they are actually smart in their very own way and I'm not sure we have actually fully reached that level of intelligence yet. But for the purpose of this definition, the idea is the ability to do a wide range of things well is a characterization of intelligence. Now my colleague Shane Legg is very passionate about the definition of intelligence. So passionate that he sifted through over 70 definitions of intelligence before he arrived at his own synthesis. And his definition is intelligence measures an agent s ability to achieve goals in a wide range of environments. You see how that s connected to the Heinlein quote and to what we think of as intelligent behavior. Now why is this important? And if we want to create artificial intelligence, we better have some kind of idea how to measure success, how to know when we have an intelligent agent. Now Shane also has mathematics at his heart, close to his heart. And so he has a formal theory of this definition of intelligence. And we will not go into the details but I would just like to briefly point it out. So this measure of intelligence here on the left is a function of pi, a policy, where a policy determines what action to take in a given state. Now this measure of intelligence is expressed as the sum over environments. And this term represents the breadth of all the things that an intelligent agent should be able to do. And he formalizes this in the framework of algorithmic information theory. And so he talks about the sum overall, computable environments. Now we need something that indicates success. And that s this term here, the value that policy pi creates in environment mu. So how successful is that policy when we expose it to that particular task or environment? And here's this thing in the middle is a complexity penalty, a waiting term. K of mu is the commogorov complexity of the environment mu. And so what this says is that if this complexity is low, then this term is great. And if this complexity is high, then this term will be smaller. And so the definition gives more weight to simple environments and then progressively less weight to more complex ones. Of course, there are many more complex environments than simple ones. And so it also acts as a normalization. Now the notion of the value and the policy come from an ii framework called reinforcement learning. And in many ways, we think of it as a very general purpose framework for AI. The idea is that there's an agent and the agent interacts with an environment. And that environment poses some kind of task or problem to the agent, if you like. And so the agent here represented as a neural net observes the state of that world and can then take an action in that world and influence the world. And once it has taken that action, it receives the subsequent observation, what has happened as a consequence of that action. And it receives a reward symbolized by the star here. Is there some kind of positive impact on the agent, some measure of success? And the goal of this agent is to learn a policy, the pie from the previous definition, such that it maximizes long term reward. So ideally, it doesn't just go for the immediate reward, but it plans ahead. It tries to act in such a way that in the long term, it will be successful in this environment. Now the beautiful thing is that this framework is so general, some people would say overly general, that it encompasses things like unsupervised learning and supervised learning as special cases. But if you want to learn more about it, there is actually a module that colleagues of mine are teaching at UCL here on reinforcement learning. So this framework will be important going forward in those first two applications of deep learning that I'm going to talk about. And the reason is that the combination of deep learning and reinforcement learning, we also rethought it as deep reinforcement learning is such a powerful combination that we can use to solve interesting interactive problems out there in the world. Now we started a lot of our work with games, and you might be aware of the work on Atari games very early on. And why do we do that? First of all, games are a bit like this reinforcement learning. You interact with this world and try to solve problems. Really often they're a microcosm of the real world. If you think about typical games, they are about value. Monopoly is about money and buying and selling. And chess is about, has spatial dimensions and time built into it. And this is a war game. They have been designed to stimulate intelligence. The designers of these games specifically wanted to stimulate human intelligence. So clearly they must have some aspect to them that is of interest when we want to build intelligence. The great thing is we can simulate games. We can set up simulations, large scale computer simulations, and learn very quickly by playing these games. And finally, games are great for us to measure progress. Because often there's some kind of winning or score or success measure associated with games. So just think of video games with a little score indicator. You know, you want to ramp up that score. That's great. That's a way of measuring progress. And in the context of our L of reinforcement learning, it can also serve as a reward. So that's what it looks like then if we apply deep reinforcement learning to a game, in this case, PON. You know, the ideas are the same. The agent observes the environment and takes actions. And then the score here is the reward that the agent gets, the reward that it's trying to maximize in the long term. And then in deep reinforcement learning, the policy here, the thing that decides in a given state what action to take, just to get up, just to get down, and so on, based on seeing this pixel image in this case. That is parameterized by a neural network whose parameters were trying to adapt so that the agent has success in the long term. For example, optimizing a long term discounted reward. So in this particular application, my colleagues ran these reinforcement learning algorithms over close to 50 different Atari games and achieved superhuman level in a lot of these games. And then I just want to use this as an example for how to think about Shane's definition. You remember there was this sum over environments and the value that the agent generates in these environments. Now think of the set of environments as these games. And then if we have an algorithm that can do well in all of these games, then according to Shane's definition, we might be tempted to call that agent to have acquired some degree of intelligence. Of course, not anywhere close to human intelligence. But you know, just that ability to solve many different tasks to a high standard as the hallmark of intelligence. Okay, so what's the role exactly of deep learning here? Well, in previous machine learning work prior to the deep learning wave, if you like. For every problem that you wanted to solve with machine learning, you first needed to define features that describe the state of the problem. You know, for documents that would be bag of words, features, and for visual problems, there would be particular filters that people defined edge detectors and so on. And the new thing with deep learning, not new now, but you know, it was back then, is to enable end to end learning to put the raw features, the pixels, the raw description of the problem in and learn the desired input output mapping, just give them the loss, how you measure success and the architecture of your neural network. That's really what I would call the definition of deep learning. Now one beautiful thing about deep learning is that through the architecture, we can put prior knowledge into the solution of our problem. And that makes learning easier. In other words, it requires less training data if we can do that. And hence also less compute. And we'll talk a little more about this later. But this prior knowledge could, for example, be about space and be about time, those basic Kantian notions, if you like. Now what makes deep learning possible and attractive now? I would argue it's the great computational power that we have available now, GPUs, TPUs, and so on. It's the large amount of data that we now have generated by mobile devices, online services, distributed sensors, labels generated by crowdsourcing by people. And finally, our better understanding of algorithms and architectures. And there's a great opportunity here because a lot of these algorithms are out there. On GitHub, you can download them and play with them. And a lot of the papers are on archive, as soon as they're written, people upload them on the archive. And there's a huge repository of information there to get started in deep learning. Good. Okay, I would like to move on now to these case studies and start off with AlphaGo and Alpha0. Some of you may have heard about these projects. And so I hope I can deliver some more details on these and give you the general gist. The core paper that I want to talk about is this paper, a general reinforcement learning algorithm that masters chess, show, and go through self-play learning with my great colleagues, David Silver, Toma Ubea, Julian Schritt, Thieser, and Janis Antenoglu and others. What you see here is the scene where Adja, who we also call the hand of AlphaGo, because he acts as AlphaGo's hand when he plays. And here, Lisa doll on the other side in this 2016 match, which was captured in this Netflix documentary that you might want to check out if you haven't seen it. So what's the problem with Go? Go is a beautiful game, complex with beautiful strategies. Doesn't take long to learn, but a lifetime to master. And the problem is that there are so many different moves in any given position. There are 361 vertices on which you can take turns to place black and white stones to surround territory. And there's just so many different ways in which games can develop. And that's where deep learning kicks in. And in particular, we use two neural networks to reduce the size of the search space, the space of possible games in which we need to do our planning. The first one, we call the policy network. And the policy network takes as input a raw goal position, characterized by empty black and white points on this 19 by 19 grid, and maps it to a probability distribution over moves. So given a goal position, this thing gives you a probability for each move being played in a particular position. Now we have a second neural network. We call the value network. And the value network also takes a given position. But it just produces one number, basically. The evaluation of that position. Is this position good for black or is it good for white? And how was this trained in AlphaGo? Well, we were lucky. We had access to a lot of human game records that people had recorded from very strong players. And so the first thing we could do was imitation learning. We could use deep learning to learn the policy network, to really just learn to imitate the human players. And that gave us the weight for the policy network. The network observes a position. It observes the professional or the highly skilled move played in that position. And now it's a simple mapping from the input board representation to that label, if you like. Now at that point, we had a policy network that was able to play in a similar way to very strong human players. It could imitate them. So we could use that neural network to very quickly generate more games. And so we generated a lot of games that then allowed us to train the value network. Because what the value network requires is, again, an input representation for the board. And the outcome of the game. Did black win or did white win? And from very many such pairs, it can then learn the probability for any given position for black or white to win. And this is already a form of reinforcement learning because we're learning the value function that I talked about earlier. Now how do we use these neural networks? We actually use them much in the way that humans would use their intuition when they approach the game. The problem is there's this huge search tree when you expand from a given position all the different ways black can play, then all the counters by white and black and white and so on. It's a huge search space and it would be hopeless to just try and plan within that space if you didn't have any guidance. And these two neural networks, they give us that guidance. The policy network allows us to be smart about the moves that we choose. We don't need to check all the moves that start from this position. We can focus on the promising ones, on the ones where the professional or strong goal player would be likely to play. And that biases the search in the right direction. Now the problem that remains is that the game tree is still very deep. The typical game of goal can last 200 moves, 250 moves even longer sometimes. So how do we deal with that? That's where the value net comes in. Because we don't need to go all the way to the end of the game to observe its outcome if black wins or white wins. We can stop somewhere in the middle after a few moves and use the trained value network to give us an estimate of how good the position is for black or for white. And so together the policy network and the value network reduce the size of this huge search tree and allow us to traverse it and find good plans in it. And that's what AlphaGo does. And it worked. We weren't always sure it would work. But in 2016 we had the match against Isidol, a phenomenal nine-dand professional goal player from Korea. And at that point no program had ever beat a professional player at that level in a match. And in a very exciting match AlphaGo ended up winning four games to one. And if you want to share that excitement, I really recommend that you take a look at this Netflix documentary AlphaGo, which details our past there and also the drama of the match. Good so much for AlphaGo, but we weren't quite happy with that because AlphaGo is just one game, right? And so we said that intelligence requires us the agent to be able to solve more than one game, maybe three games, you know, not much, but a little better. And so I'd like to talk about Alpha0 and how that manages not only to play two more games, but also use much less human knowledge. Because remember AlphaGo was still using those professional game records and also some additional features that we had designed. And so what's interesting is that a short story by Stefan Swijk, the Royal Game, can give us some insight into how Alpha0 approaches this problem. You see here on the right Stefan Swijk, the author. And the book tells the story of Dr. B, an innocent man who has been arrested and is being held in solitary confinement. Not unlike our learning agents, Dr. B is alone in his small world and starved for stimulation. I quote, they did nothing other than subjecting us to complete nothingness. For as is well known, nothing on earth puts more pressure on the human mind than nothing. While waiting for an interrogation, Dr. B manages to steal a book from one of his captors a book about the game of chess. Ego to engage his mind, Dr. B devours the book and learns to play chess. On a makeshift board in his cells, he replaced the master games from the book over and over again. But after a few weeks, the games from the book have lost their novelty, desperately looking for further diversion, Dr. B attempts to play chess against himself. But he soon realized that he can only play against himself if he splits his mind into two halves. An eye black and an eyewide. Only now with two agents in play, true interaction and learning can happen. Years later on a cruise ship, Dr. B meets the world chess champion of the time, one Mercosentovitch, an expert at chess and only at chess. In his stunning demonstration of his skills, Dr. B manages to do the impossible. He wins at chess against the world chess champion. Now fast forward 80 years and Stefan Psyck's story becomes reality in a way that not even the author could have imagined and he could imagine a lot. The modern centovitch, Stockfish, the world computer chess champion 2016, a good old fashioned AI for playing chess and only chess. The modern Dr. B I would argue, Alpha Zero, an artificial agent that learns to play chess solely by playing against itself. Now here you see some results as white. Alpha Zero wins almost 30% of its game against Stockfish and as black it manages to draw most of the time and even wins a few more games than Stockfish does. You have to imagine Stockfish is a good old fashioned AI program that has been designed by people, by chess experts and so on and uses an enormous number of heuristics to cut down the search tree and uses all kinds of domain knowledge about chess. Now here you see the development of a time as Alpha Zero trains. This is the, these are thousands of steps and you see here the ELo number that's how we measure success here and after roughly four hours of training Alpha Zero surpasses Stockfish in its chess skills. So how does this work? The trick of course is the form of reinforcement learning and self play as you may have inferred from the story. Alpha Zero is also all alone in some sense playing chess against itself and here's how it works. So initialized with the policy and value network Alpha Zero plays by evaluating the search tree from a given position and making its best move. Then taking that next position again taking policy and value net and three search to evaluate its next move and so on and so forth. It plays and plays and plays and generates a lot of games at its current level of chess which is very low at the beginning because it's just starting and P and V are at this point almost random. But then now we have games generated and we can train the policy network because now we have a position and we know which move was made and we can train the policy network to imitate that move. We can do that for the next position and the next position and so on. The move made by Alpha Zero previously is the label from which the policy network learns. It's basically imitating itself augmented by search. Similarly we can train the value network and predict the winner of these games because we know those games we've played them all the way to the end. So for a given position we know how it will end and we can train in your network that estimates that. Now finally we put these new policy and value networks into the tree search and generate new games. Let Alpha Zero play against itself but now at a higher level because the new policy network and the new value network are better and hence together with the tree search they generate better moves and we can generate higher quality games of chess. That's how it works and it actually works not only for chess but also for goal and for show key and these are just comparisons to how long it takes to reach the levels of the more traditional contenders for being the best programs in this space. Now one thing that I found very interesting is how does Alpha Zero reason about chess positions in order to appreciate that you need to understand that a classical chess program on the left evaluates tens of millions of positions before it makes every move. It expands the tree to that many nodes. Now Alpha Zero only expands about tens of thousands of positions less by a factor of a thousand so it searches much more focused and of course that still is far cry from how human grand masters operate because they only evaluate hundreds of positions. So intuition for chess is so great that they are very good at both selecting the lines they look at and evaluating the resulting positions. But you see here that in some sense we've made a move from this brute force approach towards the smarter way of solving these problems that humans employ. One thing I like is that we actually discovered some chess knowledge here or not we actually Alpha Zero did. For example traditional openings like the English opening was discovered by Alpha Zero and it continues to play it. There's other openings that are also known to humans that are discovered but then discarded you know not good enough. Alpha Zero understood that line although it's been played by humans for a long time you know just not good enough. I want to give you one example of play of Alpha Zero. This is my favorite game it's called the immortal Tsuktsvang game. Tsuktsvang is a German word that indicates a situation in which it's not actually advantageous for a side to move but they'd rather stay still than do nothing. The rules of chess don't allow that. So what you see here is Alpha Zero is white and stockfilge is black and you see here that whites pieces are very active and black is very crammed into this corner with the queen in the corner blocked by that rock. The king also blocked in these two rocks protecting that horn it doesn't look good and now we can take a look at which moves actually or which pieces black cannot move in this situation and the tragedy of the position if you like is that moving any of these pieces leads to a loss for black. Do you see that Alpha Zero has a real appreciation of these positional advantages the mobility of the pieces and dominating the board. Okay, let's conclude. The learning helps us conquer this huge search space and self-play produces the large amount of data that we need to train these deep neural networks. There's also another thing at play which we call an automatic curriculum because at the beginning the system starts to play in a very weak fashion and as it becomes better it's opponent also becomes better and so it always trains against opponent that's just of the right level because it's training against itself and that automatic curriculum leads to stronger and stronger play and the system discovers new knowledge which I think is a beautiful property of AI systems. There's still many open questions it's just a game and relatively small spaces as opposed to real world situations but it's an interesting first step. There's more material that you can look at if you're interested. Now, let's move to a more action-filled game learning to play the game of capture the flag. This is based on a recent science paper with Max Jederberg, Wojtek Tranetski and Ian Dunning and it's about playing the game of capture the flag. So it's a large scale decentralized multi-agent learning problem because capture the flag is an objective game where multiple agents need to learn to interact to play it well and you see here first person perspective and here a top-down view of a typical game situation. How does this game work? Well, we play it as a 2v2 game. You run to the opponent base and pick up the flag. You want to bring it back to your own base but you need to make sure that when you capture it that your own flag is at your own base. Let's take a look at this from the game's perspective here. This is the agent perspective and this is the top-down perspective and you'll see how this game works. So you see the blue agents there. They're going towards the red flag and capture it and they want to now bring it back to their own base. And the trick is that you really need to have your flag at your base to score and that's why you need coordination. For example now their flag has been stolen by the red agent and so they need to tag that red agent and get their flag back before they can score a flag of their own. Now the types of environments that you see here we have two. We have an outdoor version, one that's placed in some kind of desert setting and here we have an indoor version. We wanted to show that the system can handle these two types of very different terrains. And specifically what's interesting here is we use procedural generation. Whereas you and I we would probably play this game on the same map every time or maybe you just change maps a few times what we require the agents to do here is to play on a different map every single time. And here you see a sample of these maps. They all look different. And what that does is when the agents learn to play on these varied maps they learn to generalize. They learn robust strategies that work in all kinds of maps rather than just road learning a particular map. And so the trick is also to train a population of agents and you see here our training setup. There's a population of agents down here and they connect to what we call arenas. Each one of these is a little game simulation and some sample of these this population connects to these games for agents to on blue side to on red side. They play they learn they get feedback they win or they lose and then that stream of experience is rooted back to those agents and that's where the newer networks are updated and they learn. And they all train pretty much independently other than through their interaction in the arena. Here's the neural network architecture that we use. It's a two level hierarchy of recurrent neural networks a fast one and a slow one so we can cover different time scales. And it also learns about the reward signal because this problem is very difficult if you only ever get a reward signal at the end. You know imagine you play a five minute game of capture of the flag and in the end someone tells you oh you just won or you just lost and you now need to figure out going back in the game what was it that made me win or what was it that made me lose wouldn't it be much better if you had some intermediate rewards like oh I just tagged the opponent that must have been a good thing or I just got the flag of the opponent. And so that's what happens at this top level when we learn those rewards. We also use a population of agents because that allows us to get some robustness towards different playing styles. Different agents learn to play the game in different ways and so when agents train against these different agents they again need to develop robust strategies in order to succeed in this game. So here you see the results and again we measure agent skill in elo. So the higher it is the better and this is how they develop over time. You see this is the baseline a random agent that doesn't do much it's just jittering about you know randomly. Naive's self-play doesn't quite work as you can see here it's almost as bad as the random. Here is the baseline of an average human and here's the baseline of a strong human and you see that the deep learning agent developed in this work learns and learns and learns surpasses the average human surpasses the strong human and ends up with a much higher skill than all of them. Now one thing that we found particularly nice is when we tested with humans we also let them fill in some questionnaires and we asked them so which ones of your teammates was most collaborative you know with whom could you work the best and it turns out that the human players indicated that they liked it best to play with the AI. They found it reliable and strong and you know they wanted to play with the AI. So a nice finding. Now I'd also like to use this example to show you something that is maybe a little underappreciated and it's the idea of understanding how these trained agents that behave in these arguably quite clever ways in these environments represent the world. And in order to do that we've here done a teesni embedding like a two-dimensional embedding of the internal states of the agents as they play this game and we can color the points by our knowledge of the game situation. For example here we know that these points represent situations in which the agent flag is at the base the opponent flag is at their base and the agent is in their home base. You know we know that because we can look at the game but the agents also know that you know they know that this is a particular type of situation and they are represented by internal activations that are all similar to one another and that are different from a different situation like here where the agent flag is taken and the opponent flag is held by the teammate and so somehow in their internal representations they've learned what a given situation is like and as a consequence they can use that to make the make good decisions about how to play in those situations. And good decisions they make here are some patterns of behavior that we've observed and here for example home base defense they try to defend their own home base they sometimes camp in the opponent home base you know they just wait there until the flag responds and they can they can steal it or they can follow the teammate and really just work together in the same area and it turns out that this most advanced agent is actually most similar in playing style to to the humans that we observed playing. Now I just want to emphasize that there's a whole dimension of research out there where we not only train these systems but we look at their internal representation how do they represent the world and we look at their behavior and really understand how they solve the problem so we can learn from that but also ensure that they behave in safe ways for example if it's a safety critical problem. Good now we've seen that in this more complex multiplayer game the agents can clearly reach human level it takes a lot of compute to achieve that no doubt training populations is important because you want a diverse training signal and the self-play that still worked for alpha goal for alpha zero didn't actually work here because they just learned one strategy and there wasn't enough diversity and robustness in there. The second thing of course that makes things robust is that we have diversity of environments because they we procedurally generate them and we can begin to understand how these agents behave and why. This papers and blog posts on this that you can read to pick up more detail. Now as the third case study I would like to go beyond games and talk about how we can use deep learning to learn to fold proteins. The particular project I'll talk about is called alpha fold and the paper is called improved protein structure prediction using potentials from deep learning and is worked by my colleagues Andrew senior Richard Evans John jumper and James Kirkpatrick and many others. So what is this about? What is this protein folding? First let's understand what proteins are. Some of you may know more about this than I do and but he is the just they're the fundamental building blocks of life. They carry out all kinds of functions in our bodies. They catalyze reactions. They transduce signals across the cell membrane. They regulate genes. They do cellular transport. They provide antibodies and they're very important for clinical drugs. Often they are the target of particular drugs but also many drugs are proteins and the key thing we need to know to understand a protein is its shape. What the shape of a given protein? On the right here you see this amazing animation of proteins in action and you can see they really act as molecular machines. They can fulfill an amazing diversity of functions in the body depending on their shape. Now the interesting thing is that in some sense the specification of a protein is just a chain of amino acids. It's a sequence of amino acids from an alphabet of 20 different amino acids. And what happens is that these amino acids they interact locally to form these shapes for example helices or sheets. And then these helices and sheets they interact more globally to form the 3D shape of the overall protein. And then proteins can interact to do all of these amazing things. Now the problem that we want to solve is that of protein structure prediction. And you have to imagine that these proteins they have a backbone. It's basically the kind of the main chain that determines their shape. And they have these little side chains that influence how this backbone interacts with itself as it folds back on itself. And if we can figure out from the sequence you know this is the sequence of amino acids. What the 3D shape of the protein is then we can understand what this protein does because it acts when it's in this 3D shape. You can also imagine that if we have a shape in mind that we want to create it would be really good to have this kind of mapping available because then we could invert it and from the desired shape devise the sequence of amino acids that we would need to create in order to build the thing that folds into that particular shape. It's known as the inverse problem. Now how can we think about the shape of these proteins? The goal really is to predict for every atom in this protein exactly where it ends up when it folds. But one way of parameterizing this is through the torsion angles. And once we have all these two entorsion angles which are the angles how these different bonds connect to one another then we know the 3D shape. Just imagine if you wanted to determine the shape of these things someone told you which way they rotate at each point and you would be able to figure out where they are. So those are the parameters that we're looking for. Once we have these angles then we should be done. But there's a paradox and that is called Leventhal's paradox and it's basically the following. Many naturally occurring proteins fold reliably and quickly to their native state despite the astronomical number of possible configurations. So how if there are so many ways in which these things can fold how do they find the right way so that they end up in exactly the shape that's necessary in the living organism. And I've done a little example here. So suppose we have a chain length of 361 amino acids and just at any point there are three different ways in which they could fold then we would have three to the power of 361 is roughly 10 to the 172 configurations in which they could fold. Now imagine these proteins can wiggle really quickly and they can explore 10 to the 13 a different configurations per second or 10 to the 20 per year. That seems like really quick search through that space right but it would still take 10 to the 152 years to sample all the possible configurations. It's a huge space and for those of you who know the game of Go I've chosen 361 as the example on purpose here because that's the number of vertices on a go board and three to the power of 361 is an upper bound on the number of legal go positions of all the kind of ways in which you can have a go position. And John Trump actually calculated how many legal positions there actually are in Go and you see I tend to get sidetracked to Go because I just love the game and he represents that number as a ternary number and a ternary number because any of these points can have three states empty black or white can be represented as a go position and so this is the go position that as if you read it as a ternary number represents the number of legal positions in the game of Go. Now you might be wondering is it a legal position? It is an illegal position. You know there's a black stone here where it shouldn't be and another one here and that's actually accurate because the majority of these configurations of a go board are in fact not legal go positions. But back to Leventel's paradox there's a huge search space but we know that deep learning can do something about deep search spaces. So let's do that. So why do we want to use deep learning for protein folding? There are experimental methods of course to determine the structure of proteins but it's a very difficult modeling problem. We have data available, 150,000 proteins in the protein data bank which was founded in 1971 so this is a long ongoing process but we have much less data than for some of the other tasks like speech recognition or image recognition. So it's a little harder. There's another advantage. There is CAS which is an assessment, some kind of competition that provides a benchmark for protein folding. So there's a way of testing how well a system does. So what should we predict? It turns out that the 3D structure of such a proteine is fully described by a dis-parawiz distance matrix. So if you have all of these points in space, if you know all the pair-wise distances between them then you know what that configuration in space looks like. And so the main thing that's being predicted in this system is this distance matrix. Conveniently it looks a bit like an image and images were good at addressing with deep learning. Here's how the alpha-fold system works. The sequence comes in here and we generate certain features from databases. So it's not quite just using the raw data but it's pulling in features about these sequences from databases and then it does its distance predictions, also some angle predictions and it produces a score function. And this score function is a number that measures for a given folding configuration for that sequence how good that folding configuration is. And it's differentiable. And if it's differentiable we can do gradient descent and we can optimize it and that's really the key idea behind this work. I will not go into the details of this deep dilated convolutional residual network but my colleagues in the future lectures will discuss architectures in greater detail and you will have no trouble understanding what's going on here once you've been through their lectures. That's just one thing to be said it's a very very deep neural network with 220 of these blocks one after the other. Good how accurate are the predictions. The first thing we can compare is ground truth of this distance matrix with the estimates of the system and we find that the system does a good job at capturing not only short term interactions but also long range interactions. And then we can compare the foldings themselves and these are good examples where the folding worked well but you can see here that blue which are alpha-folds predictions and green are reasonably well aligned. So the system understands the gist of these proteins and how they fold. The second step is this gradient descent that we can do. Now that we have a potential and energy function if you like that we want to optimize we can use gradient descent on these angles and just wiggle down in this configuration space and then you see that here in action. It's literally optimizing the coordinates of all the different bits and folding in the process trying to optimize the score that it has estimated to be a good potential for this problem. You started multiple times to discover local optima but overall it's a gradient descent operation. Now I said earlier that one reason this was such a nice setup for deep learning is that there's a good assessment exercise and this is Casp in this case Casp 13 and it is a competition in which there are 82 chains that fold into some 3D structures but they're not known to the community yet. They are kept secret and so these chains are released one chain per day for a period of time and then the participating teams have three weeks to return five guesses or predictions of what they think that particular chain will fold into. It's a very popular competition over 90 groups from different labs across the world participate and then they have a particular scoring mechanism which boils down to measuring how close the predictions are to the ground truth that has been determined by those experimental techniques. This is a fantastic piece of work designing this competition and we're in depth to decades of work here of those people who run the competition or the participants but of course also those people who do the experimental work of producing this data because that's hard work. Some people estimate that for some proteins it takes an entire PhD thesis to get this 3D structure and entire PhD thesis. That's a lot of work. Okay so here are the results. These are the different teams that participated and in fact the deep learning system comes out as the best performing system in this exercise. So this deep learning based distance prediction clearly works. It gives more accurate predictions of contact between residues although it estimates distance previously a lot of systems use contact when they're close enough to run another closer than eight astrums but it also delivers rich information because you can imagine there's more information in a distance than in the binary signal or if something is together or further apart right we have more information and finally because it's a smooth prediction and we get these distances it's real numbers it's also a smoother potential that is easier to optimize and we see that's why the gradient descent for finding the configurations works. There are many limitations still of course the accuracy of the system is still limited it doesn't work so well on some proteins or protein templates. The method depends on what we pull out of that database so only because there's a database of similar structures can we get the features that allow us to make those predictions and also it only predicts the backbone and we then use a tool called Rosetta to fill in the side chains. So it's one small step in a problem that has been thought about and worked on for many decades but it shows that deep learning has something to contribute to these problems in science and specifically in biology. Good so that was the three case studies that I wanted to present. AlphaGo the classic if you like with its extension into Alpha0, a board game, a video game, the game of culture of the flag with more players, more richer interactions harder to process visuals and so on and finally an example from the world of science where deep learning can really make a contribution to scientific progress in a field where that matters because it might help develop new cures. Now I'd like to use the rest of the time to give you an overview of the field of deep learning by going through the different lecture topics that my colleagues are going to talk about and here they are and we start with number two because we just in the process of doing number one. The first lecture or after this one is on the foundations of neural networks and will be delivered by Voitech Genetski and it will really answer the questions what are neural networks, what kinds of functions can they represent, how are they trained, you know, back propagation, some review of those ideas, many of you will be familiar with them but I can tell you when Voitech explains these things then they become clearer than they have ever been and of course also what are the limitations of neural networks. Limitations of neural networks have led to the first neural network winter many decades ago when neural networks with only a single layer were not able to solve the simple x-or problem and when only adding a second layer to the neural network then enables the system to solve this kind of problem. So that's these things are important to know. The second lecture after that or number lecture number three will be on convolutional neural networks for image recognition given by Sander Dileman and the idea here is really to pick up on the on the thought that we want to imbue the neural network with a form of prior knowledge because that will make learning more efficient or data efficient and introducing convolutional neural networks is one way of doing that. Convolutions encode a particular weak prior about how images behave. For example they can encode translation invariant, you know, no matter where an object is in the image it will still be the same and we'd like to encode these things in our neural networks and convolutional neural networks brainchild of Jan Lecun in his little net five work. They really revolutionized image recognition because they made these neural networks competitive for these tasks and nowadays every neural network application in the area of vision makes use of convolutional neural networks. Now in the next lecture the recapitration will talk about vision beyond image net and object recognition and talk about more advanced models and this might involve object detection, semantic segmentation, you know, what is what in a given scene estimation of optical flow. She will also talk about analysis of videos. Videos can be viewed just as stacked images in time and there are interesting tasks there. For example action recognition. From a single frame you might not know what someone is doing but if you see a video of it then there's an action going on you might want to recognize what that is, you know, for example someone smashing in the window or doing sports exercise. The next thing is self supervised learning. One problem that deep learning is plagued with is that it's supervised very and depends on labels, you know, when we learn object recognition we need a photo and a label what's in it so to speak. But in self supervised we can learn a lot about the world without labels and this is particularly interesting if you have multiple modalities. Think of a video on YouTube there's a video stream but there's also an audio stream and the audio can tell us something about the video and the video can tell us something about the audio and so we can learn representations from that. The next lecture is on optimization for machine learning given by James Martens. So you can think of optimization as the engine that drives the learning process and of course it's a very old field how to optimize functions and he will focus in particular on gradient based optimization methods and their application to training neural networks. He'll cover gradient descent, momentum methods, second order methods and stochastic methods and just to give you an example this is a visualization of the error surface of the loss surface of a of a neural network. You see it's a complex beast. It has this fine substructure everywhere with local optima and there's a super interesting result in particular in this paper he mode connectivity that there are all of these little optima, these local optima but there's a path between them where when you walk along per in the parameter space along this path all the solutions along that path are relatively good solutions that generalize well and those connect those modes and you know there's a lot of complexity here in those loss surfaces and we need to understand optimization to to become good at training these neural networks. We'll then move on to sequences and recurrent networks delivered by Marta Garnelo and this is again the idea that we want to imbue the neural network with prior knowledge but this time it's knowledge about time, it's knowledge about sequences maybe the idea that in a sequence for what happens now more recent stuff matters more than stuff that happened longer time ago and why is this important? Well if you think about it a lot of data comes in the form of sequences speech, text, DNA sequences, video, audio, they're all sequences we might think of vectors that's like the first thing that we learn in machine learning but almost all the interesting stuff comes in sequences and so she will discuss recurrent neural networks and also the famous LSTM's long short term memory which is a way of dealing with the problem of vanishing gradients in training recurrent neural networks. Another interesting task here is sequence to sequence learning. Suppose you want to translate one language to another that's one sequence to another and so there's there's ways of training neural networks to do that as well. We'll then move on to deep learning for natural language processing in some sense a special case of of the previous work and Felix will discuss why deep learning is a good technique for language and he'll discuss simple recurrent neural networks applied to language but also more complex models up to transformers which is one of the most successful models. He'll also talk about unsupervised learning because not every piece of text is labeled so what can we learn if we just have a dump of Wikipedia or have read it you know a vast amount of text can we learn something about language from that without having labels and finally he'll also talk about subtle things like situated language understanding you know what does the situation of a particular agent tell us the grounding the interaction of language in the world. Here's just a tiny example of some recent very exciting work in language modeling you know where train this huge model I think on the on the reddit corpus of a lot of data and it's a system that you can prompt with them input text and it'll then produce a continuation of that text and it produces very fluent text I'll just read the completion but you know it's about the prompt is about scientists discovering unicorns you know pretty absurd but the model just keeps going the scientist named the population after the distinctive horn of each unicorn these four horn silver wide unicorns were previously unknown to science you know it's kind of superficially makes sense you listen to it and it sounds like English right what's going on. So then Alex Graves will talk will turn your and his attention to attention and memory in deep learning which are emergent topics that are very important we know of course that for human cognition attention and memory are very important but can neural networks embody these ideas. It turns out that they can and you can even for a normal neural network see that it will pay particular attention if you like to some parts of the input rather than others to solve particular tasks we call that implicit attention but we can also make that explicit we can build mechanisms that allow the neural network to zoom in on particular parts of the input to put its attention there if you like and the highlight of that lecture might well be the idea of the neural Turing machine or differentiable neural computer where there's a neural network controller that has access to an external memory that can write to it and read from it and it can solve problems in this way for example it can write the graph of the of the London underground system into this memory and then answer questions about it about the connectivity of that graph. So that should be an interesting one then move to unsupervised learning in particular generative latent variable models and variational inference delivered by Andre Nee and I already mentioned unsupervised learning is very important because we don't have labels for a lot of tasks and in particular an interesting model and that you will consider is the autoencoder model at the variational autoencoder model and the idea here is that from data you might want to infer some latent variable responsible for generating that data for example there's the image of a digit and you might want to determine what the label of that digit is but then given the label of that digit you might want to generate images of there was digits and training these jointly is the model of the variational autoencoder and it's a very powerful model for unsupervised learning when you don't have labels but when you need your system to learn the underlying representation. Then in lecture 10 we'll continue with work on unsupervised learning with the focus on representations and this will be delivered by me E. L. A. Rostka and Irina Higgins and they'll ask the question what is a good representation which of course is task dependent but can we characterize how we would like to represent the world and they'll argue that unsupervised learning has the potential to address a lot of the open problems that deep learning is struggling with for example the large amount of data of label data that is needed and they'll discuss different approaches to this here's a little example there's a data set of 3D projections of 3D chairs and you know they just come in and the system is just confronted with those how would it structure its perception of those it turns out that this beta variational autoencoder is able to find disentangled dimensions in this chaos where for example here's a dimension along which the rotation of the chair is discovered as one independent dimension the width of the chair is discovered as another independent dimension and the leg style is discovered as yet another independent dimension so this algorithm is thrown at this collection of data and it discovers what we would call natural factors in this data and that of course can be very useful for downstream tasks in lecture 11 we'll talk about generative adversarial networks this is delivered by me here E. L. A. Rostka and Jeff Donahue and this is a particularly fascinating recent development namely a model a generative model for data that is based on a little game that is deep play it's really multi-agency a two-agency there are two players the generator that generates data and it describes data that tries to describe me find out if what we generate a generated if that's a genuine image from the data or it's just something that there was made up by the generator and by the way is leading together in gradient space if you like the discriminator becomes better and better at distinguishing what the generator generates from real world data but the generator becomes better and better at fooling the discriminator to generate data that really looks like real data and there are now many variations of this it's in good fellow started this work in 2014 and it's it's one of the very interesting and hot topics in deep learning now finally and I'm very happy that we have this in the program the last lecture will be on the topic of responsible innovation and will be given by Jrsson Gabriel and Chong Li Jin and the thought here is that AI provides powerful tools that are shaping our lives and our society but that of course with the great power that we can wield here also comes great responsibility and we would like to address that in this final lecture in two ways the first one is about building safe robust and verified AI systems that do what we expect them to do for example here you see an example of an adversarial example where a classifier classifies this image correctly as a deer but if you add just a tiny bit of noise to it the right kind of noise adversarial noise it will then misclassify almost the same image as a bird so these systems are not robust and we need to understand the boundaries of what these systems can do and there's a whole field developing around this idea that we need our AI systems to be a lot like normal engineering reliable and robust and so on and we're beginning to understand how to do this better and better and the second aspect of this lecture is how to think about the ethical consequences of building AI systems and more as a joke and a starting point I've put here the three laws of robotics by Asimov the science fiction writer because you know already 80 years ago he was thinking about the consequences of deploying AI systems in the world and had some idea of how they would need to be thought about the ethics of this and what kinds of laws they should be following and of course now we are confronted with systems that actually do influence the world you know you have questions of employment where there's automation and AI systems can now do a lot of the jobs that humans can do more and more in the future probably you have the questions of bias and fairness that these systems inherit certain properties from the data sets that they learn from that we might not agree with and that we might want to change maybe an opportunity to change some things here and so Yassin will talk about these aspects of of our work in AI to close the series of lectures okay that's all I had to say thank you very much now we have some time for questions do you have any yes please I wonder your thoughts on what the most important limitations are on deep learning yes so the question is what are the most important limitations of deep learning and if there are any there are plenty I would say it's an emerging technology if I had to name my number one it would be the lack of data efficiency so with all of these examples I've shown we had a lot of data available in these reinforcement learning scenarios we generate a lot of that data ourselves but in other settings we might need to draw on existing data sets and they might just be limited in size for example in the protein folding situation we have these 150,000 sold at proteins with a lot of duplicates but that's not actually that much data for what deep learning requires and my colleagues used a lot of data augmentation techniques to squeeze out the maximum of information out of this so that would be my number one data efficiency because as humans we're also we're very data efficient in our learning and if we could approach that with deep learning I think that would be great another one of my concerns that is a little related to is energy consumption because these computational systems also consume a lot of energy and we know that our brain works on roughly 20 watts like that's a dim light bulb so how can we bridge that gap that's another one of those and then on the side of AI I think there's way to go in questions like flexibility and common sense you know what we might characterize as fluid intelligence our ability to quickly adapt to a new question to a new situation to understand it to act appropriately those are the big areas to work on plenty of PhD CCCs out there or preface that you were put forward and was intelligence with all these different options and all the different types of intelligence how are these four scenarios or case studies helping achieve that wider goal that you preface at the beginning? Yeah so I think they go in the right directions but they're really only the the beginning of the path the goal you you ask you correctly characterize is that we want this generality these many different things and how do do these settings help us in the Atari games there clearly is a set of very different games that characterize you know that that require very different behaviors so there is some diversity there is it enough diversity you know ideally we would have a less constrained setting more real world noisy with more different challenges that's true but I think you have to see the trend you know for example in alpha go we started with a single game and a lot of human knowledge that came into it and then we generalized it to three different games that the system can now play and of course it could be trained on other games as well so we expanded the class of things and I think the game we're playing here is we want to always expand the class of things that we can do with our available methods and and eventually have a very large pool of things that can be done. Can I ask one follow up question do you believe automated driving in terms of driving as cars you think that if we can solve that there's still a somewhere away from that I guess we can do that on the motorways and highways but all the different scenarios are you looking at the generality the nuances common sense the judgment we need to be operating with the environment you think we solve that you have a long way on to solving the purpose that you have at the beginning. Yeah that's a good question so does autonomous driving take us somewhere near a real intelligence because it requires the right reaction in so many different scenarios another way that we think about this is autonomous driving AI complete is it true that if you can flawlessly drive and have a self-driving car does that mean that you have a full intelligence and I think it's it's kind of in between but because there's still come some constraints there but it's it's almost like that because there's such a wide variety of different scenarios as you're describing and currently systems don't have the flexibility to react to these so I don't think the right approach here is to just add data and try to sample that space of situations we need to come up with new ideas how the system can reason about about the world you know maybe with physical models maybe with multi-agent models because a lot of the predictions we do in traffic are based on our understanding of what the other agents want you know does this agent want to discover want to overtake me what does that bicycle ride I want to do do they want to turn so all of these things so I think as we improve our understanding of how to model physical systems how to model multi-agent systems and how to acquire common sense then we'll be approaching autonomous driving it's a beautiful example because it's kind of it's so clear what we want to do here in autonomous driving yet it is so hard other questions yes please that's a good question are we entering into the new cast competition I don't know um but um my colleagues are certainly very passionate about the problem and work has continued on protein folding for us this protein folding this this was one step in that work and but we're in that for the long term so they are certainly interested yes please yeah we've brainforce learning you have two agents learning what for the other somehow isn't there a reason that actually they may not explore the form of space since it smells super vaxx right yeah so the question is if we do reinforcement learning with say two agents that train against one another will they explore the whole space of possibilities or could they be get stuck in some particular part of strategy space so that's that's a very real concern even in a single agent reinforcement learning setting the problem of exploration versus exploitation is a huge problem so in order to reach long term reward maximize long term reward the agent needs to explore the system and figure out what works and what doesn't but at the same time it can then not exploit what it already knows about the system because if it only exploits it will not find out about other parts of that space so even in single agent our l exploration is a huge problem in a two player self play kind of scenario it can be a problem but we were able to overcome it with a bit of randomization in alpha zero that somehow led led it to explore a lot of other options but in the capture the flight work we went one step further and we created a pool of diverse players that play differently and and that was in an attempt to address exactly the problem you're pointing out because that forces a single player then to have a strategy that can deal with a number of other opponent strategies and even teammate strategies and that's the same type of robustness that that we're looking for in other intelligent systems you know be able to deal with a large variety of situations be that environments or opponents or teammates that's exactly the question how can you verify that how can you check the space is broken yeah again that's a very good question how do we measure that one way of measuring it is by doing experiments with humans because humans are incredibly good at figuring out the patterns and finding counter strategies and that's why we did both in alpha go and alpha zero but also in the capture the flag work we benchmarked with humans and they did find some interesting strategies an alternative is to train yet another agent that is just designed to exploit the given agent that that we're trying to test it's just designed to find the weaknesses in it and if reinforcement learning works then that agent will find weaknesses in it and in the alpha star work where my colleagues applied deep learning and RL to the game of Starcraft they had explicit exploiter agents in the pool they were training against whose sole role was to exploit the given agents that's being trained to avoid it from developing degenerate strategies any other questions yes it seems to me that correctly specifying the reward function reinforcement learning is very important especially in cases where you get intermediate rewards for subgoals such as in the capture the flag case where you get reward for tagging the pair because if you've got too many points for that your agent might decide to ignore the game and just throw around tagging enemies yeah and if that's true how do you think we could get to a point where we have a more general approach where you guys don't have to look at the game in very specifically create more function that works right yes that's a good question so it is a big problem how do we specify a reward function such that the resulting system that optimizes them actually solves the problem that we're interested in and the the answer is sometimes simple in a game of go that last you know you want to win rather than lose so there is clear in the game of capture the flag it's also clear you also want to win that game but it's such a sparsary reward signal that you want intermediate rewards and the way we did it in that work I didn't dwell on that is we actually learned a weight for the different game events that we can observe you know capture a flag tag in a opponent be tagged the off-lag being stolen and so on there's a whole list of game events those are the events that real players of this game gets points for in the game and we learned a waiting function for these and the waiting function was learned to optimize for the final outcome go and so there is a way of bootstrapping the learning process from the final outcome to denser more informative reward signals that the RL agents can then pick up one it's one way of solving it I think okay I'm afraid we need to wrap up our terms up I hope we can welcome you to the next lecture on the basics of newer networks by a Vartek have a great evening thanks